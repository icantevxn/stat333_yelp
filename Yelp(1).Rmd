---
title: "Yelp"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r reading}
# Read data and clean up some R formatting issues
yelp_train = read.csv("Yelp_train.csv")
yelp_test = read.csv("Yelp_test.csv")
yelp_validate = read.csv("Yelp_validate.csv")

ncol(yelp_train)
ncol(yelp_test)
ncol(yelp_validate)

colnames(yelp_train)
colnames(yelp_test)
colnames(yelp_validate)

# since there is one additional column which is star in yelp_train, make a temporary file of it without column 3 which is the row of stars
yelp_train_temp = yelp_train[,-3]

yelp_2combined = rbind(yelp_test,yelp_validate)
yelp_3combined = rbind(yelp_train_temp,yelp_test,yelp_validate)

```


```{r cleaning}
# Some basic data cleaning
# get rid of the first column, which is the original sample ID
yelp_train = yelp_train[,-1]
yelp_2combined = yelp_2combined[,-1]
yelp_3combined = yelp_3combined[,-1]

```


```{r converting}
# convert text into actual strings
yelp_train$text = as.character(yelp_train$text)
yelp_2combined$text = as.character(yelp_2combined$text)
yelp_3combined$text = as.character(yelp_3combined$text)

```


```{r nword}
# Distribution of nword by star rating
plot(range(yelp_train$nword), c(0,0.011), main="Distribution of nword", xlab="Number of Words", ylab="Frequency", type='n')
colpalette = c("red","orange","green","turquoise","blue")
for (i in 1:5){
  subsamples = yelp_train$stars==i
  d = density(yelp_train$nword[subsamples])
  lines(d, col=colpalette[i])
}
legend("topright",legend=levels(factor(yelp_train$stars)), fill=c("red","orange","green","turquoise","blue"))
```


```{r nchar}
# Distribution of nchar by star rating
plot(range(yelp_train$nchar), c(0,0.0022), main="Distribution of nchar", xlab="Number of characters", ylab="Frequency", type='n')
colpalette = c("red","orange","green","turquoise","blue")
for (i in 1:5){
  subsamples = yelp_train$stars==i
  d = density(yelp_train$nchar[subsamples])
  lines(d, col=colpalette[i])
}
legend("topright",legend=levels(factor(yelp_train$stars)), fill=c("red","orange","green","turquoise","blue"))
```


```{r sentiment score}
# Relationship between sentiment score and star rating
meanscore = rep(0,5)
names(meanscore) = 1:5
for (i in 1:5) meanscore[i] = mean(yelp_train$sentiment[yelp_train$stars==i])
barplot(meanscore, xlab='Stars', ylab="Average sentiment score")
```


```{r given words distribution}
# plotting the word count against star rating
plotWordStar = function(stars, wordcount, wordname){
  meancount = rep(0,5)
  names(meancount) = 1:5
  for (i in 1:5)    meancount[i] = mean(wordcount[stars==i])
  barplot(meancount, main=wordname, xlab="Stars", ylab="Average word count")
}

par(mfrow=c(2,4))

for (i in 17:ncol(yelp_train)){
  plotWordStar(yelp_train$stars, yelp_train[,i], colnames(yelp_train)[i])
}
```


```{r given words pvalue}
# testing if a specific word count is associated with star rating
new_pvals_given = rep(0,97)
names(new_pvals_given) = colnames(yelp_train)[17:ncol(yelp_train)]
for (i in 1:97){
  ctable = table(yelp_train$stars, yelp_train[,i])
  new_pvals_given[i] = fisher.test(ctable, simulate.p.value = T)$p.value
}
new_pvals_given
```


```{r most frequently used}
# combine all the review texts from all three files
text_3combined = paste(unlist(yelp_3combined$text), collapse =" ")

# find the top 500 frequently used words
library(tm)

docs = Corpus(VectorSource(text_3combined))
docs

docs = tm_map(docs, removePunctuation)
docs = tm_map(docs, content_transformer(tolower))
docs = tm_map(docs, stripWhitespace)

dtm = TermDocumentMatrix(docs)
m = as.matrix(dtm)
v = sort(rowSums(m),decreasing=TRUE)
d = data.frame(word = names(v),freq=v)
frequent_words_and_frequency = head(d, 5000)
frequent_words = as.character(frequent_words_and_frequency$word)


```


```{r only select adjectives and adverbs}
library(tidytext)
library(tidyverse)
interested_words = unnest_tokens(tibble(txt=frequent_words),word, txt) %>%
  left_join(parts_of_speech) %>%
  filter(pos %in% c("Adjective","Adverb")) %>%
  pull(word) %>%
  unique
interested_words
```

```{r}
different_words = setdiff(interested_words,colnames(yelp_train)[17:ncol(yelp_train)])
different_words
```


```{r new words distribution}
# select the predictors from interested_words

# generate some new predictors
library(stringr)
different_words =unlist(different_words, use.names = F)
new_X = matrix(0, nrow(yelp_train), length(different_words))
colnames(new_X) = different_words
for (i in 1:length(different_words)){
  new_X[,i] = str_count(yelp_train$text, regex(different_words[i], ignore_case=T)) # ignore the upper/lower case in the text
}

par(mfrow=c(2,4))
for (i in 1:length(different_words)){
  plotWordStar(yelp_train$stars, new_X[,i], colnames(new_X)[i])
}

```


```{r new words pvalue}
# testing if a specific word count is associated with star rating
new_pvals_new = rep(0,length(different_words))
names(new_pvals_new) = different_words
for (i in 1:length(different_words)){
  ctable = table(yelp_train$stars, new_X[,i])
  new_pvals_new[i] = fisher.test(ctable, simulate.p.value = T)$p.value
}
new_pvals_new
```


```{r count the number of words for the new words}
# Generate word count for the entire dictionary of words
# generate some new predictors
library(stringr)
#different_words = c("reasonable", "cheap", "average", "dry", "bland", "slow", "wrong", "special", "clean", "happy", "enough", "ill", "cold", "disappointed", "kind", "full", "typical", "not good", "bad service", "good service", "busy", "cheap", "poor", "omg", "wtf", "lol", "wow", "love", "never", "highly recommended", "stuffed", "yum", "!", ":\\)", ":\\(")

for (i in 1:nrow(yelp_train)){
  for (j in 1:length(different_words)){
      yelp_train[i,different_words[j]] = str_count(yelp_train$text[i], regex(different_words[j], ignore_case=T)) # ignore the upper/lower case in the text
  }
}


```

```{r count the number of words for the new words for yelp_2combined}
# Generate word count for the entire dictionary of words
# generate some new predictors
library(stringr)
#different_words = c("reasonable", "cheap", "average", "dry", "bland", "slow", "wrong", "special", "clean", "happy", "enough", "ill", "cold", "disappointed", "kind", "full", "typical", "not good", "bad service", "good service", "busy", "cheap", "poor", "omg", "wtf", "lol", "wow", "love", "never", "highly recommended", "stuffed", "yum", "!", ":\\)", ":\\(")

for (i in 1:nrow(yelp_2combined)){
  for (j in 1:length(different_words)){
      yelp_2combined[i,different_words[j]] = str_count(yelp_2combined$text[i], regex(different_words[j], ignore_case=T)) # ignore the upper/lower case in the text
  }
}

```


```{r test using train}

dat = yelp_train[,-c(1,3:5,9,13)]
benchmark = lm(stars~., data=dat)
summary(benchmark)

```

```{r lasso}
library(glmnet)

Xmat <- model.matrix(benchmark)[,-1]
Ymat <- dat$stars
yelp.lasso <- glmnet(Xmat,Ymat)
plot(yelp.lasso, xvar = "lambda", label=TRUE, lwd=1, main="Lasso Regression"); abline(h=0, lwd=1, lty=2, col="grey")
##legend("topright", lwd=1, lty=1, legend=colnames(Xmat))

set.seed(1) #Lasso penalty, choosing lambda by cross-validation
yelp.lasso.cv <- cv.glmnet(Xmat, Ymat, nfold=10)
plot(yelp.lasso.cv)

yelp.lasso.cv$lambda.min

slope.lasso <- coef(yelp.lasso.cv, s = "lambda.min")[-1]
slope.lasso

selected <- which(slope.lasso!=0)
colnames(Xmat)[selected]

predict.test = yelp_2combined[,-c(1:4,8,12)]

predict(yelp.lasso.cv, yelp_2combined, s = "lambda.min")

```


```{r}
star_out <- data.frame(Id=yelp_2combined$Id, Expected=predict(benchmark, newdata = yelp_2combined))
write.csv(star_out, file='14_Submission.csv', row.names=FALSE)
```

```{r}
temp= read.csv("14_Submission.csv")

nrow(temp)
nrow(yelp_test)
nrow(yelp_validate)

```

